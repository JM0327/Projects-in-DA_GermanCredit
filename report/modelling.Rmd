# 3 Data preparation for models

```{r, echo = FALSE, message = FALSE}
source(here::here("scripts/setup.R"))
```

## 3.1 Train set and test set

```{r}
set.seed(123)
index.tr <- createDataPartition(y=Credit$RESPONSE,p=0.8,list=FALSE)
Credit.tr <- Credit[index.tr,]
Credit.te <- Credit[-index.tr,]
```


```{r}
table(Credit.tr$RESPONSE)
```

# 4. supervised learning 

## 4.1 classification tree
```{r}
credit.tree <- caret::train(RESPONSE ~ .,
                           data = Credit.tr,
                           method ="rpart",
                           preProcess = NULL,
                           trControl=trainControl(method="repeatedcv", number=10,
                                                  repeats=10, verboseIter=FALSE,
                                                  sampling="up"))

rpart.plot(credit.tree$finalModel)
credit.tree
```


```{r}
Credit.pred <- predict(credit.tree,newdata=Credit.te)
confusionMatrix(data=Credit.pred, reference = Credit.te$RESPONSE)
# draw a confusion matrix plot later
```

## 4.2 logistic regression
- unbalanced data    

```{r}
logr.bf <- glm(RESPONSE~., data=Credit.tr[2:32], family="binomial")
summary(logr.bf)
```

```{r}

logr.prob<- predict(logr.bf,newdata=Credit.te)
logr.pred <- ifelse(logr.prob >= 0.5, 1, 0) %>% as.factor()
confusionMatrix(data=logr.pred, reference = Credit.te$RESPONSE)

```

```{r balancing data}
set.seed(123)
n.yes <- min(table(Credit.tr$RESPONSE)) 

Credit.tr.no <- filter(Credit.tr, RESPONSE== "0") ## the "No" cases
Credit.tr.yes <- filter(Credit.tr, RESPONSE== "1") ## The "Yes" cases

index.yes <- sample(size=n.yes, x=1:nrow(Credit.tr.yes), replace=FALSE) 

Credit.tr.subs <- data.frame(rbind(Credit.tr.no, Credit.tr.yes[index.yes,])) 
table(Credit.tr.subs$RESPONSE) 
```

- balanced data with cross-validation
```{r}
set.seed(123)
logr.af <- caret::train(
  RESPONSE~.,
  data = Credit.tr.subs[,-1],
  method="glm",
  family="binomial",
  trControl=trainControl(method = "cv",
                         number = 10), 
  trace=0)
logr.af
```

```{r}
logr.pred2<- predict(logr.af,newdata=Credit.te)
confusionMatrix(data=logr.pred2, reference = Credit.te$RESPONSE)
```


# 5. Unsupervised learning    

## Clustering   

#### **Hierarchical clustering**

**Distance**
```{r, echo = FALSE, message = FALSE, warning=FALSE}
hierarchical.d <- dist(Credit.tr[,-32], method = "euclidean") 

hierarchical.melt <- melt(as.matrix(hierarchical.d))

kable(hierarchical.melt[1:50,], caption = "Example of the Euclidean distance between instances") %>%
  kable_styling(bootstrap_options = "bordered") %>%
  kableExtra::scroll_box(width = "100%", height = "250px")
```

**Dendrogram**   

```{r, echo = FALSE, message = FALSE, warning=FALSE, fig.height=4, fig.width=6}
hierarchical.cluster <- hclust(hierarchical.d, method = "complete")
plot(hierarchical.cluster, hang=-1)
```

**Choice of the number of clusters**

```{r, echo = FALSE, message = FALSE, warning=FALSE, fig.height=6, fig.width=8}
c.p1 <- fviz_nbclust(Credit.tr[,c(-1,-32)],
             hcut, hc_method="complete",
             hc_metric="euclidean",
             method = "wss", 
             k.max = 25, verbose = FALSE)

c.p2 <- fviz_nbclust(Credit.tr[,c(-1,-32)],
             hcut, hc_method="complete",
             hc_metric="euclidean",
             method = "silhouette", 
             k.max = 25, verbose = FALSE)

# c.p3 <-fviz_nbclust(Credit.tr,
#              hcut, hc_method="complete",
#              hc_metric="euclidean",
#              method = "gap", 
#              k.max = 25, verbose = FALSE)
c.p1 / c.p2 
```

```{r, echo = FALSE, message = FALSE, warning=FALSE, fig.height=4, fig.width=6}
plot(hierarchical.cluster, hang=-1)
rect.hclust(hierarchical.cluster, k=2)
```

```{r, echo = FALSE, message = FALSE, warning=FALSE}
clust.h <- cutree(hierarchical.cluster, k=2)
clust.comp <- data.frame(Credit.tr[,c(-1,-32)], 
                              Clust=factor(clust.h),
                              Id=row.names(Credit.tr[,c(-1,-32)]))

cluster.df <- melt(clust.comp, id=c("Id", "Clust"))
```

**Interpretation of the clusters**

```{r, fig.height=20, fig.width=20, echo = FALSE, message = FALSE, warning=FALSE}
ggplot(cluster.df, aes(y=value, group=Clust, fill=Clust)) +
  geom_boxplot() +
  facet_wrap(~variable, scale='free')+
  theme(legend.text=element_text(size=rel(4)),
        strip.text=element_text(size=17),
        axis.text =element_text(size=17))
```

#### **Partitioning methods**    

**Partitioning Around the Medoid (PAM)**   

```{r, out.width="50%", echo = FALSE, message = FALSE, warning=FALSE, fig.height=4, fig.width=6}
fviz_nbclust(Credit.tr[,c(-1,-32)],
             cluster::pam,
             method = "silhouette", 
             k.max = 25, verbose = FALSE)
```

```{r, echo = FALSE, message = FALSE, warning=FALSE, fig.height=4, fig.width=6}
pam <- cluster::pam(Credit.tr[,c(-1,-32)], k=2)
plot(cluster::silhouette(pam), border = NA, col = 1:2)

```

